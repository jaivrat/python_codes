{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial 12 - Activation Functions\n",
    "\n",
    "https://www.youtube.com/watch?v=3t9lZM7SS7k\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do not have activation functions, then our model will be just a linear regression model. It wont be suited for complex (non linear fits)\n",
    "\n",
    "<img src=\"images/activation.png\">\n",
    "\n",
    "\n",
    "So we apply activation fucntions over linear transform  $w^T x$\n",
    "Most popular activation functions are\n",
    "\n",
    "1. Step function\n",
    "\n",
    "2. sigmoid\n",
    "\n",
    "3. tanh\n",
    "\n",
    "4. ReLU\n",
    "\n",
    "5. leaky relu\n",
    "\n",
    "6. softmax\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/step-function.png\">\n",
    "\n",
    "\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "<img src=\"images/sigmoid.png\">\n",
    "\n",
    "#### Tanh\n",
    "(values between -1 and + 1)\n",
    "<img src=\"images/tanh-function.png\">\n",
    "\n",
    "\n",
    "#### ReLU\n",
    "(Most popular)\n",
    "<img src=\"images/relu-function.png\">\n",
    "\n",
    "#### Leaky ReLU\n",
    "\n",
    "<img src=\"images/leaky-relu-function.png\">\n",
    "\n",
    "\n",
    "#### Softmax\n",
    "<img src=\"images/softmax-function.png\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two ways to use the layers and activation functions \n",
    "\n",
    "#### Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "### option 1\n",
    "class NeuralNet3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet3, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2\n",
    "(use activation functions directly in forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### option 2 (use activation functions directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet3, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
