{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial 16 - How To Use The TensorBoard\n",
    "\n",
    "https://www.youtube.com/watch?v=VJW9wU-1n18\n",
    "\n",
    "We can use tensorboard to experiment with our models - visualize. It is deveoped by tensorboard but can ye used by pytorch as well\n",
    "https://www.tensorflow.org/tensorboard/\n",
    "\n",
    "We start with code from tutorial 13 - which does digit classification\n",
    "\n",
    "On commandline (after pip install tensorboard)\n",
    "```\n",
    "$tensorboard --logdir=runs\n",
    "```\n",
    "Like this\n",
    "\n",
    "```\n",
    "(base) jvsingh: -> tensorboard --logdir=runs\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.3.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "When we open in browser - we do not see any data as we have not written anything as of now\n",
    "\n",
    "<img src = \"images/tsboard-no-data.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write data for tensorboard, we need to set up writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cpu\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device:{device}\")\n",
    "# hyper parameters\n",
    "input_size = 784 # 28 x 28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST\n",
    "train_dataset = torchvision.datasets.MNIST(root = \"./data\", \n",
    "                                           train = True,\n",
    "                                           transform = transforms.ToTensor(),\n",
    "                                           download = True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = \"./data\", \n",
    "                                          train = False,\n",
    "                                          transform = transforms.ToTensor(),\n",
    "                                          download= True)\n",
    "\n",
    "# we shuffle\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                           batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "\n",
    "# we dont shuffle\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                           batch_size = batch_size,\n",
    "                                          shuffle = False)\n",
    "\n",
    "\n",
    "\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = examples.next()\n",
    "print(example_data.shape, example_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(6):\n",
    "#    # 2 rows and three columns\n",
    "#    plt.subplot(2, 3, i + 1)\n",
    "#    # i th sample \n",
    "#    # 0: because we want to access first channel(we have only one)\n",
    "#    # cmap: colour map\n",
    "#    plt.imshow(example_data[i][0], cmap=\"gray\")\n",
    "\n",
    "# Rather than plotting - we will add our images to tensorboard \n",
    "image_grid = torchvision.utils.make_grid(example_data)\n",
    "writer.add_image('mnist_images', image_grid)\n",
    "writer.close() # Dlushes outputs\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running above will write images into tensorfboard input folder and you can refresh tensorboard to see the images\n",
    "<img src=\"images/ts-board-images-appear.png\" width=400>\n",
    "\n",
    "\n",
    "We will also add to writer as below ( see after setting optimizer below). After running below you can see\n",
    "\n",
    "<img src=\"images/ts-board-graphs.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (images, labels) in enumerate(train_loader):\n",
    "#    print(f\"i={i} images.size={images.size()[0]}\")\n",
    "#\n",
    "#print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "937 * 64 + 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, step 100/938 loss=0.5760\n",
      "epoch 1/5, step 200/938 loss=0.2235\n",
      "epoch 1/5, step 300/938 loss=0.2829\n",
      "epoch 1/5, step 400/938 loss=0.1187\n",
      "epoch 1/5, step 500/938 loss=0.1124\n",
      "epoch 1/5, step 600/938 loss=0.0829\n",
      "epoch 1/5, step 700/938 loss=0.1824\n",
      "epoch 1/5, step 800/938 loss=0.1426\n",
      "epoch 1/5, step 900/938 loss=0.2102\n",
      "epoch 2/5, step 100/938 loss=0.1518\n",
      "epoch 2/5, step 200/938 loss=0.1374\n",
      "epoch 2/5, step 300/938 loss=0.1223\n",
      "epoch 2/5, step 400/938 loss=0.1375\n",
      "epoch 2/5, step 500/938 loss=0.0711\n",
      "epoch 2/5, step 600/938 loss=0.0945\n",
      "epoch 2/5, step 700/938 loss=0.1751\n",
      "epoch 2/5, step 800/938 loss=0.0765\n",
      "epoch 2/5, step 900/938 loss=0.0800\n",
      "epoch 3/5, step 100/938 loss=0.0319\n",
      "epoch 3/5, step 200/938 loss=0.0652\n",
      "epoch 3/5, step 300/938 loss=0.0692\n",
      "epoch 3/5, step 400/938 loss=0.0635\n",
      "epoch 3/5, step 500/938 loss=0.0381\n",
      "epoch 3/5, step 600/938 loss=0.1839\n",
      "epoch 3/5, step 700/938 loss=0.0209\n",
      "epoch 3/5, step 800/938 loss=0.0759\n",
      "epoch 3/5, step 900/938 loss=0.1061\n",
      "epoch 4/5, step 100/938 loss=0.0322\n",
      "epoch 4/5, step 200/938 loss=0.0373\n",
      "epoch 4/5, step 300/938 loss=0.0352\n",
      "epoch 4/5, step 400/938 loss=0.0202\n",
      "epoch 4/5, step 500/938 loss=0.0249\n",
      "epoch 4/5, step 600/938 loss=0.0175\n",
      "epoch 4/5, step 700/938 loss=0.0472\n",
      "epoch 4/5, step 800/938 loss=0.0906\n",
      "epoch 4/5, step 900/938 loss=0.0118\n",
      "epoch 5/5, step 100/938 loss=0.0287\n",
      "epoch 5/5, step 200/938 loss=0.0128\n",
      "epoch 5/5, step 300/938 loss=0.0231\n",
      "epoch 5/5, step 400/938 loss=0.0522\n",
      "epoch 5/5, step 500/938 loss=0.0128\n",
      "epoch 5/5, step 600/938 loss=0.0348\n",
      "epoch 5/5, step 700/938 loss=0.0307\n",
      "epoch 5/5, step 800/938 loss=0.0126\n",
      "epoch 5/5, step 900/938 loss=0.0068\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  #applies softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# For viewing in Tensorboard\n",
    "writer.add_graph(model, example_data.reshape(-1, 28 * 28))\n",
    "writer.close()\n",
    "#sys.exit()\n",
    "# For viewing in Tensorboard\n",
    "\n",
    "#trainign loop\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # we have to reshape the image : 100, 1 , 28, 28\n",
    "        # we need 100, 784   (784 = 28 x 28)\n",
    "        images = images.reshape(-1, 28 * 28).to(device) #pushes to gpu if avilable\n",
    "        lables = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backwards \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Tutorial says zero_grad can be put in any order\n",
    "        #but only thing to make sure is that it must be \n",
    "        #called before next iteration\n",
    "        # I wonder why ? because I am thinking that optimizer.step() may be using \n",
    "        # gradient information to step.\n",
    "        # it seems that optimizer.step() might be using gradient info inside to step. So if you make it zero before stepping, then first step goes waste. However the code still works because the next step() call may be using gradient of previous iteration. So, there might be a \"loss\" of one batch of data, but still it works.\n",
    "        \n",
    "        \n",
    "        if (i + 1)% 100 == 0:\n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_total_steps} loss={loss.item():.4f}')\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5, step 100/938 loss=0.3006\n",
      "epoch 1/5, step 200/938 loss=0.1892\n",
      "epoch 1/5, step 300/938 loss=0.2882\n",
      "epoch 1/5, step 400/938 loss=0.1905\n",
      "epoch 1/5, step 500/938 loss=0.2923\n",
      "epoch 1/5, step 600/938 loss=0.2072\n",
      "epoch 1/5, step 700/938 loss=0.1655\n",
      "epoch 1/5, step 800/938 loss=0.0742\n",
      "epoch 1/5, step 900/938 loss=0.2098\n",
      "At epoch level : running_loss:241.31286326795816 running_accuracy:0.9269 \n",
      "epoch 2/5, step 100/938 loss=0.0978\n",
      "epoch 2/5, step 200/938 loss=0.0917\n",
      "epoch 2/5, step 300/938 loss=0.1434\n",
      "epoch 2/5, step 400/938 loss=0.0937\n",
      "epoch 2/5, step 500/938 loss=0.0685\n",
      "epoch 2/5, step 600/938 loss=0.1672\n",
      "epoch 2/5, step 700/938 loss=0.1848\n",
      "epoch 2/5, step 800/938 loss=0.0952\n",
      "epoch 2/5, step 900/938 loss=0.0828\n",
      "At epoch level : running_loss:96.06545485556126 running_accuracy:0.9689333333333333 \n",
      "epoch 3/5, step 100/938 loss=0.0230\n",
      "epoch 3/5, step 200/938 loss=0.0171\n",
      "epoch 3/5, step 300/938 loss=0.0644\n",
      "epoch 3/5, step 400/938 loss=0.0651\n",
      "epoch 3/5, step 500/938 loss=0.0887\n",
      "epoch 3/5, step 600/938 loss=0.0917\n",
      "epoch 3/5, step 700/938 loss=0.0874\n",
      "epoch 3/5, step 800/938 loss=0.0082\n",
      "epoch 3/5, step 900/938 loss=0.1069\n",
      "At epoch level : running_loss:62.46436664927751 running_accuracy:0.9794666666666667 \n",
      "epoch 4/5, step 100/938 loss=0.0179\n",
      "epoch 4/5, step 200/938 loss=0.0075\n",
      "epoch 4/5, step 300/938 loss=0.0442\n",
      "epoch 4/5, step 400/938 loss=0.0575\n",
      "epoch 4/5, step 500/938 loss=0.0350\n",
      "epoch 4/5, step 600/938 loss=0.0180\n",
      "epoch 4/5, step 700/938 loss=0.0266\n",
      "epoch 4/5, step 800/938 loss=0.0461\n",
      "epoch 4/5, step 900/938 loss=0.0588\n",
      "At epoch level : running_loss:43.39096807409078 running_accuracy:0.9860333333333333 \n",
      "epoch 5/5, step 100/938 loss=0.0170\n",
      "epoch 5/5, step 200/938 loss=0.0087\n",
      "epoch 5/5, step 300/938 loss=0.0836\n",
      "epoch 5/5, step 400/938 loss=0.0269\n",
      "epoch 5/5, step 500/938 loss=0.0427\n",
      "epoch 5/5, step 600/938 loss=0.0433\n",
      "epoch 5/5, step 700/938 loss=0.1001\n",
      "epoch 5/5, step 800/938 loss=0.0236\n",
      "epoch 5/5, step 900/938 loss=0.0186\n",
      "At epoch level : running_loss:31.02269520261325 running_accuracy:0.9901333333333333 \n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(\"runs/mnist1\")\n",
    "\n",
    "#WE JUST ADD TRAING LOSS and ACCURACY - we do not change anything else\n",
    "#Re create model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "#loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  #applies softmax\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "writer.add_graph(model, example_data.reshape(-1, 28 * 28))\n",
    "#writer.close()\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss      = 0.0\n",
    "    running_correct   = 0.0\n",
    "    running_incorrect = 0.0    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        #print(f\"debug- epoch:{epoch}, i={i}\")\n",
    "        # we have to reshape the image : 100, 1 , 28, 28\n",
    "        # we need 100, 784   (784 = 28 x 28)\n",
    "        images = images.reshape(-1, 28 * 28).to(device) #pushes to gpu if avilable\n",
    "        lables = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backwards\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # just to show in tensorboard\n",
    "        # if (i + 1)% 200 == 0:\n",
    "        #    print(f\"loss.item()={loss.item()}\")\n",
    "        running_loss += loss.item()\n",
    "        _, predictions = torch.max(outputs, 1) #along dimension 1\n",
    "        _correct   = (predictions == labels).sum().item() \n",
    "        _incorrect = predictions.size()[0] - _correct\n",
    "        running_correct   += _correct\n",
    "        running_incorrect += _incorrect\n",
    "        #print(f\"predictions.size: {predictions.size()[0]}\")\n",
    "        \n",
    "        if (i + 1)% 100 == 0:\n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, step {i+1}/{n_total_steps} loss={loss.item():.4f}') \n",
    "            \n",
    "    ############## TENSORBOARD ########################\n",
    "    # we also calculate mean value and add to tensorboard\n",
    "    writer.add_scalar('training loss', running_loss, epoch * n_total_steps + i)\n",
    "    running_accuracy = running_correct / (running_correct + running_incorrect)\n",
    "    writer.add_scalar('accuracy', running_accuracy, epoch * n_total_steps + i)\n",
    "    print(f'At epoch level : running_loss:{running_loss} running_accuracy:{running_accuracy} ')\n",
    "    ############## TENSORBOARD ########################\n",
    "\n",
    "writer.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see scalar in tensorboard after adding above metrics pushed thorugh **writer**\n",
    "\n",
    "\n",
    "<img src=\"images/ts-scalars.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do another run with different writer\n",
    "```python\n",
    "writer = SummaryWriter(\"runs/mnist2\")\n",
    "```\n",
    "then we will see a new line with \"minst2\" tag. This way we can compare many performances together\n",
    "\n",
    "\n",
    "\n",
    "#### Now let use see out model performances\n",
    "\n",
    "We can add precision recall curve - it makes more sense in binary classification. But if we analyse each class separately then we do have a kind of binary classication with respect to each class.\n",
    "\n",
    "In the [tensorboard link](https://pytorch.org/docs/stable/tensorboard.html#), we can search for **add_pr_curve**, which is adding the precision and recall curve. See the description there.\n",
    "\n",
    "```\n",
    "add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images = 98.06 %\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "class_labels = []\n",
    "class_preds = []\n",
    "\n",
    "# test\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28 * 28).to(device) #pushes to gpu if avilable\n",
    "        lables = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #value, index\n",
    "        _, predicted = torch.max(outputs, 1) #along dimension 1\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Note that outputs are from linear layer at the end. In NN code out = self.l2(out)\n",
    "        # , we see No activation and no softmax at the end(CrossEntropyLoss had taken care of this part while\n",
    "        # calculating loss)\n",
    "        # But in evaluation here we want actual probabilities: So here we need softmax function\n",
    "        # explicitly on the outputs\n",
    "        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "        #print(f\"debug:{class_probs_batch}\")\n",
    "        class_preds.append(class_probs_batch)\n",
    "        class_labels.append(predicted)\n",
    "    \n",
    "    # we concatenate into one dimensional tensor\n",
    "    class_labels = torch.cat(class_labels)  #Since we have 10000 samples, the shape will be 10000x1\n",
    "    class_preds = torch.cat([torch.stack(batch) for batch in class_preds]) # this will be 10000 x 10, since we have 10 classes\n",
    "    \n",
    "    acc = 100.0 * n_correct/n_samples\n",
    "    print(f'Accuracy of the network on the test images = {acc} %')\n",
    "    \n",
    "    ############## TENSORBOARD ########################\n",
    "    classes = range(10)\n",
    "    for i in classes:\n",
    "        labels_i = class_labels == i\n",
    "        preds_i = class_preds[:, i]\n",
    "        writer.add_pr_curve(str(i), labels_i, preds_i, global_step=0)\n",
    "    writer.close()\n",
    "    ############## TENSORBOARD ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
