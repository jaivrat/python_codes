{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial 17 - Saving and Loading Models\n",
    "https://www.youtube.com/watch?v=9L9jEOwRrCg\n",
    "\n",
    "We have tree different methods\n",
    "\n",
    "```python\n",
    "torch.save(arg, PATH)\n",
    "\n",
    "torch.load(PATH)\n",
    "\n",
    "model.load_state_dict(arg)\n",
    "\n",
    "```\n",
    "\n",
    "We must remember these\n",
    "\n",
    "1. **torch.save** can save any dictionary and models.\n",
    "\n",
    "2. **torch.load** \n",
    "\n",
    "3. **model.load_state_dict**\n",
    "\n",
    "\n",
    "```python\n",
    "#### 2 DIFFERENT WAYS OF SAVING\n",
    "# 1) lazy way: save whole model\n",
    "torch.save(model, PATH)\n",
    "# model class must be defined somewhere\n",
    "model = torch.load(PATH)\n",
    "model.eval()\n",
    "# 2) recommended way: save only the state_dict\n",
    "torch.save(model.state_dict(), PATH)\n",
    "# model must be created again with parameters\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "Note that ***model.eval()*** has serialized data bound to the class and exact directory struture when it was saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "model = Model(n_input_features = 6)\n",
    "\n",
    "# train your model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"model.pth\" # it is a common pratice to name with .pth extension which means pytorch\n",
    "\n",
    "torch.save(model, FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can load our model now\n",
    "model = torch.load(FILE)\n",
    "model.eval()  # set in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2125, -0.0848, -0.2367, -0.2764,  0.3437, -0.2928]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2459], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Above was a lazy way - lets see the **recommended way** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3336, -0.2853,  0.2218,  0.2969, -0.0191,  0.1007]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3542], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = Model(n_input_features = 6)\n",
    "FILE = \"model.pth\" # it is a common pratice to name with .pth extension which means pytorch\n",
    "torch.save(model.state_dict(), FILE)  # Note - we are saving model state_dict()\n",
    "\n",
    "# We will see that this models parameters which are saved , will be same as that\n",
    "# with \"NEW\" constucted model below and loaded dict of this save model\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (linear): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "loaded_model  = Model(n_input_features = 6)\n",
    "loaded_model.load_state_dict(torch.load(FILE))\n",
    "loaded_model.eval()  # set in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3336, -0.2853,  0.2218,  0.2969, -0.0191,  0.1007]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3542], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in loaded_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see above that paramters are same.\n",
    "\n",
    "Note that when we create model, it is initialized with random parameters.\n",
    "\n",
    "\n",
    "Let us try below with some learning of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.state_dict : {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "model = Model(n_input_features = 6)\n",
    "# train the model ..\n",
    "learning_rate = 0.01\n",
    "optimizer     = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# Sometimes during training - we may want to save a checkpoint \n",
    "print(\"optimizer.state_dict : {}\".format(optimizer.state_dict()))\n",
    "\n",
    "checkpoint = {\n",
    "                \"epoch\": 90,\n",
    "                \"model_state\" : model.state_dict(),\n",
    "                \"optim_state\" : optimizer.state_dict()\n",
    "             }\n",
    "\n",
    "torch.save(checkpoint, \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_checkpoint = torch.load(\"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.state_dict : {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0, 1]}]}\n"
     ]
    }
   ],
   "source": [
    "# Now we have to set up different model and optimizer once again\n",
    "epoch = loaded_checkpoint[\"epoch\"]\n",
    "\n",
    "model = Model(n_input_features=6)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0) #put 0 , later we load correct rate\n",
    "\n",
    "# This will load all paremeters in model\n",
    "model.load_state_dict(checkpoint[\"model_state\"])  \n",
    "\n",
    "# This will load optimizer state\n",
    "optimizer.load_state_dict(checkpoint[\"optim_state\"])\n",
    "\n",
    "print(\"optimizer.state_dict : {}\".format(optimizer.state_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have **same learning rate** (= 0.01) as in optimizer earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you save your model to GPU and  you want to load on CPU\n",
    "\n",
    "Remember that you must call model.eval() to set dropout and batch normalization layers \n",
    "to evaluation mode before running inference. Failing to do this will yield \n",
    "inconsistent inference results. If you wish to resuming training, \n",
    "\n",
    "\n",
    "call model.train() to ensure these layers are in training mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING ON GPU/CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 1) Save on GPU, Load on CPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), PATH)\n",
    "device = torch.device('cpu')\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "\n",
    "\n",
    "# 2) Save on GPU, Load on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "torch.save(model.state_dict(), PATH)\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Note: Be sure to use the .to(torch.device('cuda')) function \n",
    "# on all model inputs, too!\n",
    "# 3) Save on CPU, Load on GPU\n",
    "torch.save(model.state_dict(), PATH)\n",
    "device = torch.device(\"cuda\")\n",
    "model = Model(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
    "model.to(device)\n",
    "# This loads the model to a given GPU device. \n",
    "# Next, be sure to call model.to(torch.device('cuda')) to convert the modelâ€™s parameter tensors to CUDA tensors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
