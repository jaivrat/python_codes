{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "\n",
    "We try here to create data and apply the scikit learn's PCA api - and see how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Creating data\n",
    "\n",
    "Let us do in these steps\n",
    "1. Let data be 15 dimensional( ie $R^{15}$ and having N=1000 data points.\n",
    "2. We create 5 PC vectors (challenge: how to create 5 orthogonal $R^{15}$-vectors?)  - use scipy\n",
    "3. Create each data point as a linear combination of these vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ortho_group\n",
    "from scipy import random\n",
    "import numpy as np\n",
    "from numpy import random as npr\n",
    "\n",
    "npr.seed(seed=144)\n",
    "\n",
    "\n",
    "def get_k_ndim_pc_vectors(k, n):\n",
    "    x = ortho_group.rvs(dim = n)\n",
    "    return x[range(k),:]\n",
    "\n",
    "#x = get_k_n_dim_pc_vectors(15, 5)\n",
    "\n",
    "n = 3    #dimensions\n",
    "num_main_pcs = 2 #number of main explaining components\n",
    "tot = 25 #Total number of points\n",
    "pc_vecs = get_k_ndim_pc_vectors(n, n)\n",
    "\n",
    "#Though rows and columns are all orthogonal\n",
    "pc_vecs_as_rows = pc_vecs.T\n",
    "\n",
    "#pc_vecs\n",
    "print(pc_vecs_as_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -0.,  0.],\n",
       "       [-0.,  1., -0.],\n",
       "       [ 0., -0.,  1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us make sure these are orthogonal - It will be like identity vector\n",
    "pc_vecs.dot(pc_vecs.T).round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do linear combination of these  PC vectors($R^{15}$) to produce $N=1000$ points.\n",
    "Evey point will have a different linear combination. So we get **pc_loadings** of $5 x 1000$ so that we get 15 x 1000 dim matrix (whose each column will be generated point ie $R^{15}$ vector)\n",
    "\n",
    "We also add noise loadings **random_noise**, these loadings will be small as compared to actual participating PC's which are first to 5th. These noisy loadings will add in orthogonal directions to span of first five PCs (which are going to explain for most variation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npr.random_integers(low = 1, high=10, size=(num_main_pcs,tot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "[[ 0  0  2 -2 -2  3 -1 -3  0 -2  4  1 -1  1 -4  5 -1 -1 -2 -3 -4 -1  4 -4\n",
      "   1]\n",
      " [-4 -3  3  5  4  2  3  4 -1 -2  5  3 -1  4  2 -1  3 -1  3 -4  5 -3  1  2\n",
      "  -3]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvsingh/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Main PC loading coefficients for 1000 points\n",
    "pc_loadings = npr.random_integers(low = 1, high=10, size=(num_main_pcs,tot)) - 5\n",
    "print(pc_loadings.shape)\n",
    "print(pc_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_pc_loadings shape:(25, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.e+00, -4.e+00, -9.e-03],\n",
       "       [ 0.e+00, -3.e+00, -1.e-03],\n",
       "       [ 2.e+00,  3.e+00,  2.e-03],\n",
       "       [-2.e+00,  5.e+00, -4.e-03],\n",
       "       [-2.e+00,  4.e+00, -9.e-03],\n",
       "       [ 3.e+00,  2.e+00, -6.e-03],\n",
       "       [-1.e+00,  3.e+00, -9.e-03],\n",
       "       [-3.e+00,  4.e+00,  2.e-03],\n",
       "       [ 0.e+00, -1.e+00,  4.e-03],\n",
       "       [-2.e+00, -2.e+00,  3.e-03],\n",
       "       [ 4.e+00,  5.e+00,  7.e-03],\n",
       "       [ 1.e+00,  3.e+00, -2.e-03],\n",
       "       [-1.e+00, -1.e+00,  1.e-03],\n",
       "       [ 1.e+00,  4.e+00, -6.e-03],\n",
       "       [-4.e+00,  2.e+00, -7.e-03],\n",
       "       [ 5.e+00, -1.e+00, -5.e-03],\n",
       "       [-1.e+00,  3.e+00, -4.e-03],\n",
       "       [-1.e+00, -1.e+00,  8.e-03],\n",
       "       [-2.e+00,  3.e+00,  9.e-03],\n",
       "       [-3.e+00, -4.e+00,  4.e-03],\n",
       "       [-4.e+00,  5.e+00, -9.e-03],\n",
       "       [-1.e+00, -3.e+00, -2.e-03],\n",
       "       [ 4.e+00,  1.e+00,  2.e-03],\n",
       "       [-4.e+00,  2.e+00, -6.e-03],\n",
       "       [ 1.e+00, -3.e+00, -9.e-03]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orthogonal loading coefficients for 1000 points\n",
    "random_noise = random.uniform(low = -0.01, high = + 0.01, size = ((n-num_main_pcs),tot)) # npr.normal(size = ((n-num_main_pcs),tot)) * 0.001\n",
    "final_pc_loadings = np.concatenate((pc_loadings, random_noise), axis=0).T\n",
    "print(\"final_pc_loadings shape:\" + str(final_pc_loadings.shape))\n",
    "np.round(final_pc_loadings,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.99602866, -0.07097906,  0.05374832],\n",
       "       [-0.02515794,  0.80346097,  0.59482564],\n",
       "       [ 0.08540484, -0.59111119,  0.80205584]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_vecs_as_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape all_points: (25, 3)\n",
      "[[ 0.09982826 -3.20828274 -2.38684828]\n",
      " [ 0.07536229 -2.40961103 -1.78552427]\n",
      " [-2.06734955  2.26716806  1.89367878]\n",
      " [ 1.86595015  4.16146042  2.86364995]\n",
      " [ 1.89069321  3.36087089  2.26492815]\n",
      " [-3.03894021  1.39771088  1.34584042]\n",
      " [ 0.91977482  2.48676076  1.72340321]\n",
      " [ 2.88764651  3.42545027  2.21986332]\n",
      " [ 0.02553781 -0.80609015 -0.59125821]\n",
      " [ 2.04261801 -1.4666583  -1.29484876]\n",
      " [-4.10926742  3.72898042  3.1951028 ]\n",
      " [-1.0716813   2.34064161  1.83654579]\n",
      " [ 1.02128603 -0.7331701  -0.64764019]\n",
      " [-1.0971638   3.14634896  2.4283234 ]\n",
      " [ 3.93323157  1.89476389  0.96933137]\n",
      " [-4.95544374 -1.15518373 -0.33038874]\n",
      " [ 0.92017765  2.48397269  1.72718623]\n",
      " [ 1.02184339 -0.73702773 -0.64240591]\n",
      " [ 1.91737519  2.54686159  1.68441513]\n",
      " [ 3.08904834 -3.00319495 -2.53744271]\n",
      " [ 3.85752621  4.30674948  2.75163369]\n",
      " [ 1.0713106  -2.33807587 -1.84002714]\n",
      " [-4.00913734  0.51860872  0.81108896]\n",
      " [ 3.9333025   1.89427298  0.96999747]\n",
      " [-0.92136281 -2.47576986 -1.73831633]]\n"
     ]
    }
   ],
   "source": [
    "#generate all 1000 points\n",
    "all_points   = final_pc_loadings.dot(pc_vecs_as_rows) #+ random_noise\n",
    "np.savetxt(\"/tmp/all_points.csv\", all_points, delimiter=\",\")\n",
    "print(\"Shape all_points: \" + str(all_points.shape))\n",
    "print(all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_points is a **tot x n** where each column is the point as mentioned before\n",
    "\n",
    "##### ------------------------  Data generation part completes here ------------------------ #####\n",
    "\n",
    "Let us start doing PCA of **all_points** and see if we can recover the components(ie PCs and the coefficients **pc_loadings** and data back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all_points:(25, 3)\n",
      "means(1, 3):\n",
      "[[0.37208665 0.86510229 0.59561153]]\n",
      "sds (1, 3):\n",
      "[[2.48448114 2.40649585 1.7648127 ]]\n",
      "all_points shape:(25, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of all_points:\" + str(all_points.shape))\n",
    "#Standardising \n",
    "means_of_15_dims = np.apply_over_axes(lambda x, axis: np.mean(x, axis = axis), all_points, axes = 0)\n",
    "means_of_15_dims\n",
    "print(\"means{}:\".format(means_of_15_dims.shape))\n",
    "print(means_of_15_dims)\n",
    "\n",
    "sd_of_15_dims = np.apply_over_axes(lambda x, axis: np.std(x, axis = axis, ddof=0), all_points, axes = 0)\n",
    "sd_of_15_dims\n",
    "print(\"sds {}:\".format(sd_of_15_dims.shape))\n",
    "print(sd_of_15_dims)\n",
    "\n",
    "print(\"all_points shape:\" + str(all_points.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape scaled: (25, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.1095836 , -1.69266239, -1.68995827],\n",
       "       [-0.11943112, -1.36078079, -1.34922862],\n",
       "       [-0.98186948,  0.58261716,  0.73552693],\n",
       "       [ 0.60127785,  1.36977512,  1.28514399],\n",
       "       [ 0.6112369 ,  1.03709658,  0.94588884],\n",
       "       [-1.37293329,  0.22132122,  0.42510398],\n",
       "       [ 0.22044368,  0.67386714,  0.63904328],\n",
       "       [ 1.01250914,  1.06393202,  0.92035364],\n",
       "       [-0.1394854 , -0.69445058, -0.67251881],\n",
       "       [ 0.67238641, -0.96894436, -1.07119599],\n",
       "       [-1.80373841,  1.19006153,  1.4729559 ],\n",
       "       [-0.58111448,  0.6131485 ,  0.70315352],\n",
       "       [ 0.26130179, -0.66414924, -0.70446667],\n",
       "       [-0.59137114,  0.94795371,  1.03847388],\n",
       "       [ 1.43335558,  0.4278676 ,  0.21176176],\n",
       "       [-2.14432313, -0.83951361, -0.52470173],\n",
       "       [ 0.22060582,  0.67270858,  0.64118686],\n",
       "       [ 0.26152613, -0.66575225, -0.70150076],\n",
       "       [ 0.62197636,  0.69884156,  0.61695137],\n",
       "       [ 1.09357307, -1.60743981, -1.77528995],\n",
       "       [ 1.40288428,  1.43014882,  1.22167195],\n",
       "       [ 0.28143661, -1.33105493, -1.38011171],\n",
       "       [-1.7634362 , -0.14398262,  0.12209649],\n",
       "       [ 1.43338413,  0.42766361,  0.21213919],\n",
       "       [-0.5206115 , -1.38827256, -1.32247907]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = (all_points - means_of_15_dims)/sd_of_15_dims\n",
    "print(\"shape scaled: \" + str(scaled.shape))\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.1095836 , -1.69266239, -1.68995827],\n",
       "       [-0.11943112, -1.36078079, -1.34922862],\n",
       "       [-0.98186948,  0.58261716,  0.73552693],\n",
       "       [ 0.60127785,  1.36977512,  1.28514399],\n",
       "       [ 0.6112369 ,  1.03709658,  0.94588884],\n",
       "       [-1.37293329,  0.22132122,  0.42510398],\n",
       "       [ 0.22044368,  0.67386714,  0.63904328],\n",
       "       [ 1.01250914,  1.06393202,  0.92035364],\n",
       "       [-0.1394854 , -0.69445058, -0.67251881],\n",
       "       [ 0.67238641, -0.96894436, -1.07119599],\n",
       "       [-1.80373841,  1.19006153,  1.4729559 ],\n",
       "       [-0.58111448,  0.6131485 ,  0.70315352],\n",
       "       [ 0.26130179, -0.66414924, -0.70446667],\n",
       "       [-0.59137114,  0.94795371,  1.03847388],\n",
       "       [ 1.43335558,  0.4278676 ,  0.21176176],\n",
       "       [-2.14432313, -0.83951361, -0.52470173],\n",
       "       [ 0.22060582,  0.67270858,  0.64118686],\n",
       "       [ 0.26152613, -0.66575225, -0.70150076],\n",
       "       [ 0.62197636,  0.69884156,  0.61695137],\n",
       "       [ 1.09357307, -1.60743981, -1.77528995],\n",
       "       [ 1.40288428,  1.43014882,  1.22167195],\n",
       "       [ 0.28143661, -1.33105493, -1.38011171],\n",
       "       [-1.7634362 , -0.14398262,  0.12209649],\n",
       "       [ 1.43338413,  0.42766361,  0.21213919],\n",
       "       [-0.5206115 , -1.38827256, -1.32247907]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "x_transformed = StandardScaler().fit_transform(all_points)\n",
    "x_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above that Scikit's standard scalar iz just z-scoring as we do directly in **scaled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "in_data = pd.DataFrame(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = in_data.shape[1])\n",
    "pca.fit(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.63577294e-01 3.36420067e-01 2.63905025e-06]\n",
      "[0.66357729 0.99999736 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try to discover how many components are needed\n",
    "# var_ret is [0,1], which is fraction of total variance retained\n",
    "def get_num_components(in_mat, var_ret):\n",
    "    pca = PCA(n_components = in_mat.shape[1])\n",
    "    pca.fit(in_mat)\n",
    "    expl = pca.explained_variance_ratio_\n",
    "    needed_components = 1 + np.min(np.where(np.cumsum(expl/sum(expl)) >= var_ret))\n",
    "    return needed_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_by = get_num_components(in_data, 0.99)\n",
    "explained_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04461722, -0.70808837, -0.70471282],\n",
       "       [-0.99342745, -0.04298386,  0.10608626],\n",
       "       [ 0.10540973, -0.70481434,  0.7015166 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(in_data)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have **explained_by** principal components which explain the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04461722, -0.70808837, -0.70471282],\n",
       "       [-0.99342745, -0.04298386,  0.10608626],\n",
       "       [ 0.10540973, -0.70481434,  0.7015166 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components = explained_by)\n",
    "#pca.fit(in_data)\n",
    "#pca.components_\n",
    "pca.explained_variance_ratio_\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**components_** : ndarray of shape (n_components, n_features)\n",
    "Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04461722, -0.70808837, -0.70471282])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.components_[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PC vectors are orthogonal to each other\n",
    "print(pca.components_.shape)\n",
    "np.round(pca.components_.dot(pca.components_.T),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44721309,  0.64450015,  0.6201774 ],\n",
       "       [ 0.8873025 , -0.23233779, -0.39838853]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets us see if our components are perpendicular/prallel to our starting generator vectors pc_vecs \n",
    "pca.components_  #5 x 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.05682235157504861, -0.9869801505221955, -0.15047060456004563]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ pca.components_[0,:].dot(pc_vecs_as_rows[i,:]) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-90c7db4ee2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "\n",
    "def randrange(n, vmin, vmax):\n",
    "    '''\n",
    "    Helper function to make an array of random numbers having shape (n, )\n",
    "    with each number distributed Uniform(vmin, vmax).\n",
    "    '''\n",
    "    return (vmax - vmin)*np.random.rand(n) + vmin\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# For each set of style and range settings, plot n random points in the box\n",
    "# defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
    "ax.scatter(scaled[:,0], scaled[:,1], scaled[:,2], marker='^')\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
