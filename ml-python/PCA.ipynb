{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "\n",
    "We try here to create data and apply the scikit learn's PCA api - and see how it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Creating data\n",
    "\n",
    "Let us do in these steps\n",
    "1. Let data be 15 dimensional( ie $R^{15}$ and having N=1000 data points.\n",
    "2. We create 5 PC vectors (challenge: how to create 5 orthogonal $R^{15}$-vectors?)  - use scipy\n",
    "3. Create each data point as a linear combination of these vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.26291933e-01, -6.64136469e-02,  2.67134455e-01,\n",
       "        -1.76450505e-01,  6.73835035e-02,  6.50302768e-01,\n",
       "         4.22182916e-04,  4.41506706e-01, -8.54629656e-04,\n",
       "         5.28377819e-02, -1.24489436e-01, -1.70253556e-01,\n",
       "         3.36465677e-02, -2.79008814e-01, -1.94961956e-01],\n",
       "       [-2.32522371e-02, -5.29135073e-01, -4.98369098e-01,\n",
       "         1.91045813e-01, -1.09358341e-01,  1.41556933e-01,\n",
       "        -2.02376447e-01,  1.49545257e-01, -1.09663044e-01,\n",
       "         3.13232660e-01, -1.18860838e-01,  2.72085259e-01,\n",
       "        -3.63930774e-01, -5.44110605e-02,  7.47153527e-02],\n",
       "       [ 1.76075685e-02,  7.56573807e-03, -3.27797521e-01,\n",
       "         2.09019797e-01,  3.41418111e-01, -8.88363233e-02,\n",
       "        -3.75123750e-01, -5.30461823e-02,  1.11062506e-01,\n",
       "         2.79882508e-01, -9.20845356e-02, -4.84609731e-01,\n",
       "         4.45687521e-01, -9.74416013e-02, -1.95894910e-01],\n",
       "       [ 4.66118943e-01,  9.67574157e-02,  2.61439598e-01,\n",
       "        -9.30293071e-02, -3.66199430e-02,  1.00422272e-02,\n",
       "         1.79238737e-01,  1.28147011e-01, -7.99617031e-02,\n",
       "         6.60887900e-01, -6.36205336e-02, -2.67524582e-01,\n",
       "        -2.40036265e-01,  2.63441200e-01, -2.39506023e-02],\n",
       "       [ 3.44305097e-01,  1.48403331e-01, -4.16473972e-02,\n",
       "        -1.85192171e-02, -3.85851648e-01,  4.81064910e-01,\n",
       "        -1.65268136e-01, -5.00789005e-01,  2.54689466e-01,\n",
       "        -9.43681209e-03, -2.76832069e-01,  3.21617938e-02,\n",
       "         7.97821839e-02, -1.95513235e-01,  1.08296273e-01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ortho_group\n",
    "from scipy import random\n",
    "import numpy as np\n",
    "from numpy import random as npr\n",
    "\n",
    "npr.seed(seed=144)\n",
    "\n",
    "\n",
    "def get_k_ndim_pc_vectors(k, n):\n",
    "    x = ortho_group.rvs(dim = n)\n",
    "    return x[range(k),:]\n",
    "\n",
    "#x = get_k_n_dim_pc_vectors(15, 5)\n",
    "\n",
    "pc_vecs = get_k_ndim_pc_vectors(5, 15)\n",
    "pc_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do linear combination of these  PC vectors($R^{15}$) to produce $N=1000$ points.\n",
    "Evey point will have a different linear combination. So we get $pc_loadings$ of $5 x 1000$ so that we get 15 x 1000 dim matrix (whose each column will be generated point ie $R^{15}$ vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(npr.uniform)\n",
    "pc_loadings = 10 * (npr.uniform(size=(5,1000)) - 0.5)\n",
    "pc_loadings.shape\n",
    "print(pc_loadings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate all 1000 points\n",
    "random_noise = npr.normal(size = (15,1000)) * 0.01\n",
    "all_points   = pc_vecs.dot(pc_loadings) + random_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_points is a $15x1000$ where each column is the point as mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_points.shape)\n",
    "print(all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ------------------------  Data generation part completes here ------------------------ #####\n",
    "\n",
    "Let us start doing PCA of **all_points** and see if we can recover the components(ie PCs and the coeeficients **pc_loadings** and data back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "in_data = pd.DataFrame(all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = in_data.shape[0])\n",
    "pca.fit(in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us try to discover how many components are needed\n",
    "# var_ret is [0,1], which is fraction of total variance retained\n",
    "def get_num_components(in_mat, var_ret):\n",
    "    pca = PCA(n_components = in_mat.shape[0])\n",
    "    pca.fit(in_mat)\n",
    "    expl = pca.explained_variance_ratio_\n",
    "    needed_components = 1 + np.min(np.where(np.cumsum(expl/sum(expl)) >= var_ret))\n",
    "    return needed_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_components(in_data, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have 5 principal components which explain the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 15)\n",
    "pca.fit(in_data)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**components_** : ndarray of shape (n_components, n_features)\n",
    "Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
