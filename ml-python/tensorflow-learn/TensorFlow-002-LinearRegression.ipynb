{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mpl_toolkits.mpot3d'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-217e2ff009f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mpl_toolkits.mpot3d'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpl_toolkits\n",
    "from mpl_toolkits.mpot3d import Axes3D\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as  tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model \n",
    "\n",
    "\n",
    "$y(x) = a + b_1 * X_1 + b_2*X_2 + b_3*X_3 + \\sigma \\epsilon$\n",
    "\n",
    "$\\epsilon \\approx N(0,1)$\n",
    "\n",
    "$b_1, b_2, b_3 = (0.5, 0.2, 0.1) $\n",
    "\n",
    "$\\sigma=0.1$\n",
    "\n",
    "\n",
    "$X_1, X_2, X_3$ uniformally distributed in [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "\n",
    "n_points = 5000\n",
    "n_features = 3\n",
    "\n",
    "bias = np.ones(n_points).reshape((-1,1))\n",
    "low  = -1 * np.ones((n_points, n_features),dtype='float')\n",
    "high  = np.ones((n_points, n_features),dtype='float')\n",
    "\n",
    "#similated Features\n",
    "X =  np.random.uniform(low=low, high=high)\n",
    "\n",
    "\n",
    "#simulated Noise\n",
    "noise = np.random.normal(size=(n_points,1))\n",
    "\n",
    "#outputr\n",
    "weights = np.array([1.0, 0.5, 0.2, 0.1])\n",
    "noise_std = 0.1\n",
    "\n",
    "Y = weights[0] * bias + np.dot(X, weights[1:] .reshape(-1,1)) + noise_std * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46769372],\n",
       "       [ 1.24090559],\n",
       "       [ 1.41068818],\n",
       "       ..., \n",
       "       [ 1.11978003],\n",
       "       [ 1.07924581],\n",
       "       [ 0.75105445]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the data into train and test sets\n",
    "train_test_split = 4\n",
    "\n",
    "n_test = int(n_points/train_test_split)\n",
    "n_train = n_points - n_test\n",
    "\n",
    "X_train = X[:n_train, :]\n",
    "Y_train = Y[:n_train].reshape((-1,1))\n",
    "\n",
    "X_test = X[n_train:,:]\n",
    "Y_test = Y[n_train:].reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1250, 3), (1250, 1), (3750, 3), (3750, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape, X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00030061]\n",
      " [ 0.5046615 ]\n",
      " [ 0.20047932]\n",
      " [ 0.0978062 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#add colum of ones\n",
    "X = np.hstack((np.ones(n_train).reshape((-1,1)), X_train))\n",
    "\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y_train)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00030061],\n",
       "       [ 0.5046615 ],\n",
       "       [ 0.20047932],\n",
       "       [ 0.0978062 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, Y_train)\n",
    "\n",
    "np.r_[lin_reg.intercept_.reshape(-1,1), lin_reg.coef_.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00030077],\n",
       "       [ 0.5046615 ],\n",
       "       [ 0.20047921],\n",
       "       [ 0.0978062 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = np.hstack((np.ones(n_train).reshape((-1,1)), X_train))\n",
    "\n",
    "X = tf.constant(X_np, dtype=tf.float32, name = \"X\")\n",
    "y = tf.constant(Y_train, dtype=tf.float32, name = \"y\")\n",
    "\n",
    "XT = tf.transpose(X)\n",
    "\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "    \n",
    "theta_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple class for Linear Regression\n",
    "\n",
    "Implement the Normal Equation and MLE solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.05, L=0):\n",
    "        \n",
    "        #input placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_features], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1], name = \"Y\")\n",
    "        \n",
    "        #regression paramaters for analytical Solution using Normal Equan\n",
    "        self.theta_in = tf.placeholder(tf.float32, [n_features+1,None])\n",
    "        \n",
    "        #Augmented data matrix is obtained by adding a column of ones to\n",
    "        #the data matrix\n",
    "        data_plus_bias = tf.concat([tf.ones([tf.shape(self.X,)[0],1]), self.X], axis=1)\n",
    "        \n",
    "        XT = tf.transpose(data_plus_bias)\n",
    "        \n",
    "        ###########################################\n",
    "        # The normal equation for Linear Regression\n",
    "        ###########################################\n",
    "        self.theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,data_plus_bias)),XT),self.Y)\n",
    "        \n",
    "        \n",
    "        #mean square error in terms of theta = theta_in\n",
    "        self.lr_mse = tf.reduce_mean(tf.square(tf.matmul(data_plus_bias, self.theta_in)) - self.Y)\n",
    "        \n",
    "        ##############################################\n",
    "        #Estimate Model using MLE\n",
    "        \n",
    "        #Regression paramaters for the maximum likelihood method\n",
    "        # Note that there are n_features+2 parameters, as one is added for the intercept\n",
    "        # and another one for the std dev for the noise\n",
    "        self.weights = tf.Variable(tf.random_normal([n_features+2, 1]))\n",
    "        \n",
    "        #prediction from the model\n",
    "        self.output = tf.matmul(data_plus_bias, self.weights[:-1,:])\n",
    "        \n",
    "        gauss = tf.distributions.Normal(loc=0.0, scale = 1.0)\n",
    "        \n",
    "        #STadard Deviation of the Gaussian noise is modelled as a square of the last\n",
    "        #model weight\n",
    "        sigma = 0.0001 + tf.square(self.weights[-1])\n",
    "        \n",
    "        log_LL = tf.log(0.00001 + (1/sigma) * gauss.prob((self.Y - self.output)/sigma))\n",
    "        \n",
    "        #Loss is the negative log-likelihood\n",
    "        self.loss = -tf.reduce_mean(log_LL)\n",
    "        \n",
    "        #TF node defining an Op for on training set\n",
    "        self.train.step = (tf.train.AdamOptimizer(learning_rate).minimize(self.loss), self.loss)\n",
    "        \n",
    "    def generate_date(n_points = 10000,\n",
    "                     n_features = 3,\n",
    "                     weights = np.array([1.0, 0.5, 0.2, 0.1]),\n",
    "                     noise_std = 0.1):\n",
    "        \n",
    "        bias = np.ones(n_points).reshape((-1,1))        \n",
    "        low  = -1 * np.ones((n_points, n_features),dtype='float')\n",
    "        high  = np.ones((n_points, n_features),dtype='float')\n",
    "        \n",
    "        #simulated Features\n",
    "        X =  np.random.uniform(low=low, high=high)\n",
    "        \n",
    "        #simulated Noise\n",
    "        noise = np.random.normal(size=(n_points,1))\n",
    "        \n",
    "        #output\n",
    "        weights = np.array([1.0, 0.5, 0.2, 0.1])\n",
    "        noise_std = 0.1\n",
    "        \n",
    "        Y = weights[0] * bias + np.dot(X, weights[1:] .reshape(-1,1)) + noise_std * noise\n",
    "        \n",
    "        return(X,Y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_points = 5000\n",
    "    n_features = 3\n",
    "    \n",
    "    #n_features + 1 weights (one for constant feature)\n",
    "    weights = np.array([1.0, 0.5, 0.2, 0.1])\n",
    "    noise_std = 0.1\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    #Make data\n",
    "    (X,Y) = Model.generate_date(n_points = n_points,\n",
    "                                 n_features = n_features,\n",
    "                                 weights = weights,\n",
    "                                 noise_std = noise_std)\n",
    "    \n",
    "    #split the data into train and test sets\n",
    "    train_test_split = 4 #1/4 to be used for test\n",
    "    \n",
    "    n_test = int(n_points/train_test_split)\n",
    "    n_train = n_points - n_test\n",
    "    \n",
    "    X_train = X[:n_train, :]\n",
    "    Y_train = Y[:n_train].reshape((-1,1))\n",
    "    \n",
    "    X_test = X[n_train:,:]\n",
    "    Y_test = Y[n_train:].reshape((-1,1))\n",
    "    \n",
    "    #create an instance of the Linear Regression model class\n",
    "    model = Model(n_features=n_features, learning_rate=learning_rate)\n",
    "    \n",
    "    #train the model\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #Normal Equations for Linear Regression\n",
    "        theta_value = sess.run(model.theta, feed_dict={\n",
    "            model.X: X_train,\n",
    "            model.Y: Y_train\n",
    "        })\n",
    "        \n",
    "        lr_mse_train = sess.run(model.lr_mse, feed_dict={\n",
    "            model.X: X_train,\n",
    "            model.Y: Y_train,\n",
    "            model.theta_in: theta_value\n",
    "        })\n",
    "        \n",
    "        lr_mse_test = sess.run(model.lr_mse, feed_dict={\n",
    "            model.X: X_test,\n",
    "            model.Y: Y_test,\n",
    "            model.theta_in: theta_value\n",
    "        })\n",
    "        \n",
    "        print(\"====== Linear Regression with the Normal Equation =======\")\n",
    "        print(theta_value)\n",
    "        print(\"Linear regression train error:\", lr_mse_train)\n",
    "        print(\"Linear regression test error :\", lr_mse_test)\n",
    "        \n",
    "        #Now train the MLE parameters\n",
    "        for i in range(0,1000):\n",
    "            (_,loss), weights = sess.run((model.train_step, model.weights), \n",
    "                                         feed_dict={\n",
    "                                                    model.X: X_train,\n",
    "                                                    model.Y: Y_train\n",
    "                                                })\n",
    "            \n",
    "        #make test predictions\n",
    "        Y_test_predicted = sess.run(model.output, feed_dict = {\n",
    "                                                        model.X: X_test\n",
    "                                                    })\n",
    "        \n",
    "        #output std sigma is a square of the last weight\n",
    "        std_model = weights[-1]**2\n",
    "        print(\"===== Linear Regression with MLE =========\")\n",
    "        print(\"Negative Log-Likelihood\", loss)\n",
    "        print(\"MLE fitted parameters: \")\n",
    "        print(weights[0:-1])\n",
    "        print(\"Fitted std of noise: \", std_model)\n",
    "        \n",
    "        fig.plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_test[:,1], X_test[:,2], Y_test, s=1, c=\"#000000\")\n",
    "        ax.scatter(X_test[:,1], X_test[:,2], Y_test_predicted, s=1, c=\"#FF0000\")\n",
    "        plt.show()\n",
    "        \n",
    "        sess.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'distributions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-4591af7673e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-5cd5e7b53840>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m#create an instance of the Linear Regression model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-b7f577afb12b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_features, learning_rate, L)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_plus_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mgauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#STadard Deviation of the Gaussian noise is modelled as a square of the last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'distributions'"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
