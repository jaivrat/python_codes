{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing new data sets - how to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom bs4 import BeautifulSoup\\nimport urllib.request\\n\\ndef saveFile(webpage, filename):\\n    websource = urllib.request.urlopen(webpage)\\n    soup = BeautifulSoup(websource.read(), \"html.parser\")\\n    lines = soup.get_text()\\n    fh = open(filename,\"w\")\\n    fh.writelines(lines)\\n    fh.close() \\n\\n#Negative sentiment examples\\nsaveFile(webpage = \"https://pythonprogramming.net/static/downloads/short_reviews/negative.txt\", \\n         filename = \"negative.txt\")\\n#Postive sentiment examples\\nsaveFile(webpage = \"https://pythonprogramming.net/static/downloads/short_reviews/positive.txt\", \\n         filename = \"positive.txt\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if you want to save from webpage uncomment this\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "def saveFile(webpage, filename):\n",
    "    websource = urllib.request.urlopen(webpage)\n",
    "    soup = BeautifulSoup(websource.read(), \"html.parser\")\n",
    "    lines = soup.get_text()\n",
    "    fh = open(filename,\"w\")\n",
    "    fh.writelines(lines)\n",
    "    fh.close() \n",
    "\n",
    "#Negative sentiment examples\n",
    "saveFile(webpage = \"https://pythonprogramming.net/static/downloads/short_reviews/negative.txt\", \n",
    "         filename = \"negative.txt\")\n",
    "#Postive sentiment examples\n",
    "saveFile(webpage = \"https://pythonprogramming.net/static/downloads/short_reviews/positive.txt\", \n",
    "         filename = \"positive.txt\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "short_pos = open(\"positive.txt\", \"r\").read()\n",
    "short_neg  = open(\"negative.txt\", \"r\").read()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for line in short_pos.split(\"\\n\"):\n",
    "    documents.append((line, \"pos\"))\n",
    "\n",
    "    \n",
    "for line in short_neg.split(\"\\n\"):\n",
    "    documents.append((line, \"neg\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_pos_words  = nltk.word_tokenize(short_pos)\n",
    "short_neg_words  = nltk.word_tokenize(short_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562832"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_pos_words)\n",
    "type(short_pos)\n",
    "len(short_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "#We want to use top 5000 words\n",
    "word_features = list(all_words.keys())[:5000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = nltk.word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:  #note that word_features contains top 3000 words only(above)\n",
    "        features[w] = (w in words)\n",
    "    return(features)\n",
    "\n",
    "featuresets = [(find_features(doc), category) for (doc, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = featuresets[:10000] #firs 1900\n",
    "testing_set  = featuresets[10000:] #rest starting 1900th "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874015748031497"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "nltk.classify.accuracy(classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open(\"naivebayes_pos_neg.pickle\", \"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "classifier_f = open(\"naivebayes_pos_neg.pickle\", \"rb\")\n",
    "classifier_fromPickle = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874015748031497"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Accuracy\n",
    "nltk.classify.accuracy(classifier_fromPickle, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using other classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier #wrapper of nltk over scikit learn\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8031496062992126"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_classifier = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(mnb_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Some Errors in this \n",
    "\n",
    "#gaussian_classifier = SklearnClassifier(GaussianNB())\n",
    "#gaussian_classifier.train(training_set)\n",
    "#gaussian_classifier.train(training_set)\n",
    "\n",
    "##Accuracy\n",
    "#nltk.classify.accuracy(gaussian_classifier, testing_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8031496062992126"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(bernoulli_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other scikitlearn classiers\n",
    "\n",
    "#LogisticRegression, SGDClassifier\n",
    "#SVC, LinearSVC, NuSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8188976377952756"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(LogisticRegression_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jvsingh/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8031496062992126"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(SGDClassifier_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6456692913385826"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(SVC_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7559055118110236"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(LinearSVC_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110236220472441"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "##Accuracy\n",
    "nltk.classify.accuracy(NuSVC_classifier, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "All of above have their own paramaters which you can customize to imporve the acuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return(mode(votes))\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "            \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes/len(votes)\n",
    "        return conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voted_classfier = VoteClassifier(mnb_classifier,\n",
    "                                 bernoulli_classifier,\n",
    "                                 classifier_fromPickle,\n",
    "                                 SVC_classifier,\n",
    "                                 NuSVC_classifier\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110236220472441"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(voted_classfier, testing_set)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
