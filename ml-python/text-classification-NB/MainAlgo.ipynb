{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we analyse the data of classified document, hypothetically generated from generate-data.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jvsingh: ~/work/github/python_codes/ml-python/text-classification-NB  -> cat meta_data_gen_data.csv |head\n",
    "c1,c2,c3,c4,c5,filepath\n",
    "0.049028195347408016,0.3111449048955462,0.24384859975591097,0.3331713496226755,0.06280695037845944,gen_data/c4/0.doc\n",
    "0.07478597463560355,0.1482119649710673,0.15355594526672495,0.11836313227665833,0.5050829828499459,gen_data/c5/1.doc\n",
    "0.1046171560547216,0.20654904985856354,0.06603868524870048,0.233672205660047,0.38912290317796744,gen_data/c5/2.doc\n",
    "0.1364355746659322,0.1703737112860102,0.5148515953300349,0.1185609415488728,0.05977817716915001,gen_data/c3/3.doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadatafile = \"meta_data_gen_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaDF = pd.read_csv(metadatafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049028</td>\n",
       "      <td>0.311145</td>\n",
       "      <td>0.243849</td>\n",
       "      <td>0.333171</td>\n",
       "      <td>0.062807</td>\n",
       "      <td>gen_data/c4/0.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074786</td>\n",
       "      <td>0.148212</td>\n",
       "      <td>0.153556</td>\n",
       "      <td>0.118363</td>\n",
       "      <td>0.505083</td>\n",
       "      <td>gen_data/c5/1.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104617</td>\n",
       "      <td>0.206549</td>\n",
       "      <td>0.066039</td>\n",
       "      <td>0.233672</td>\n",
       "      <td>0.389123</td>\n",
       "      <td>gen_data/c5/2.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136436</td>\n",
       "      <td>0.170374</td>\n",
       "      <td>0.514852</td>\n",
       "      <td>0.118561</td>\n",
       "      <td>0.059778</td>\n",
       "      <td>gen_data/c3/3.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.198133</td>\n",
       "      <td>0.351637</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>0.187404</td>\n",
       "      <td>gen_data/c2/4.doc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         c1        c2        c3        c4        c5           filepath\n",
       "0  0.049028  0.311145  0.243849  0.333171  0.062807  gen_data/c4/0.doc\n",
       "1  0.074786  0.148212  0.153556  0.118363  0.505083  gen_data/c5/1.doc\n",
       "2  0.104617  0.206549  0.066039  0.233672  0.389123  gen_data/c5/2.doc\n",
       "3  0.136436  0.170374  0.514852  0.118561  0.059778  gen_data/c3/3.doc\n",
       "4  0.198133  0.351637  0.007328  0.255499  0.187404  gen_data/c2/4.doc"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenData():\n",
    "    def __init__(self, metadatapath):\n",
    "        self.metadatafile = metadatapath\n",
    "        self.metaDF = pd.read_csv(metadatafile)\n",
    "    \n",
    "    def getMetaData(self):\n",
    "        return(self.metaDF)\n",
    "    \n",
    "    def getCategories(self):\n",
    "        cols = self.metaDF.columns.tolist()\n",
    "        #last is file path, pop and return\n",
    "        cols.pop()\n",
    "        return(cols)\n",
    "    \n",
    "    def getAllFilePaths(self):\n",
    "        return(metaDF[\"filepath\"].tolist())\n",
    "    \n",
    "    def readFile(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        return(\"\\n\".join(lines))\n",
    "    \n",
    "    def getAllData(self):\n",
    "        #To be returned in category and data format\n",
    "        allFiles = self.getAllFilePaths()\n",
    "        data = [(filepath.split(\"/\")[1],self.readFile(filepath))  for filepath in allFiles]\n",
    "        return(data)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.049028</td>\n",
       "      <td>0.311145</td>\n",
       "      <td>0.243849</td>\n",
       "      <td>0.333171</td>\n",
       "      <td>0.062807</td>\n",
       "      <td>gen_data/c4/0.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.074786</td>\n",
       "      <td>0.148212</td>\n",
       "      <td>0.153556</td>\n",
       "      <td>0.118363</td>\n",
       "      <td>0.505083</td>\n",
       "      <td>gen_data/c5/1.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104617</td>\n",
       "      <td>0.206549</td>\n",
       "      <td>0.066039</td>\n",
       "      <td>0.233672</td>\n",
       "      <td>0.389123</td>\n",
       "      <td>gen_data/c5/2.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136436</td>\n",
       "      <td>0.170374</td>\n",
       "      <td>0.514852</td>\n",
       "      <td>0.118561</td>\n",
       "      <td>0.059778</td>\n",
       "      <td>gen_data/c3/3.doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.198133</td>\n",
       "      <td>0.351637</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.255499</td>\n",
       "      <td>0.187404</td>\n",
       "      <td>gen_data/c2/4.doc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         c1        c2        c3        c4        c5           filepath\n",
       "0  0.049028  0.311145  0.243849  0.333171  0.062807  gen_data/c4/0.doc\n",
       "1  0.074786  0.148212  0.153556  0.118363  0.505083  gen_data/c5/1.doc\n",
       "2  0.104617  0.206549  0.066039  0.233672  0.389123  gen_data/c5/2.doc\n",
       "3  0.136436  0.170374  0.514852  0.118561  0.059778  gen_data/c3/3.doc\n",
       "4  0.198133  0.351637  0.007328  0.255499  0.187404  gen_data/c2/4.doc"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDataObj = GenData(metadatapath=metadatafile)\n",
    "getDataObj.getMetaData().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1', 'c2', 'c3', 'c4', 'c5']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDataObj.getCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gen_data/c5/1.doc',\n",
       " 'gen_data/c5/2.doc',\n",
       " 'gen_data/c3/3.doc',\n",
       " 'gen_data/c2/4.doc',\n",
       " 'gen_data/c3/5.doc',\n",
       " 'gen_data/c5/6.doc',\n",
       " 'gen_data/c2/7.doc',\n",
       " 'gen_data/c1/8.doc',\n",
       " 'gen_data/c5/9.doc']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metaDF[\"filepath\"].tolist()[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "('c5', \"pixilation won't doesn't tremie polyarthritic Chrysophanus transmittant now euplastic yaourti about unchronicled Ornithomimus unpatterned more sympathicoblast isn't lithochemistry only semideity cubonavicular goblinry doesn't basihyoid seaconny young tricenarium hers polonaise thrivingness themselves few though pleb thermotank isn't Shukulumbwe off wasn when those if unbenetted unshattered guaka doing myself until reaward nor porphyrine Tyrrhenian on how young eggfish aren't mountlet tingling uniembryonate it's Niobite untroublesomeness sart uncolleged likelihood sporange unmercurial knockup thingummy dermatozoon isn't from lessener illogic bowwood proratable invendibility not young volost unpacific urobilin unwieldable unmummify freemason pronunciatory dilettantism transnational weedy calyx transposal those how both sermonism assistless entrustment metership very Moed ll suprathoracic Saguerus is coldslaw you'd hydronitric young meadsman ununanimous reweld young mansuetude athyria overapprehensive than prosaical ve for needn't insurrectory Mishongnovi during haven knifesmith testamentum uncircumcisedness shouldn't unface orthosymmetrically cacographic prohibitory young deacidify precordium semiharden young trickishness Rhabdocoelida reset azotorrhoea Dipterocarpaceae trillium criminous themselves remanation they young coadjuvancy platinoid uncoloredly havenward comparative soggarth any thermesthesiometer ain doesn't young should've down durra sternutatory up unconsecutive redisable overlaxative tritocerebrum phyma young above young subtilizer transcreate jackstone young A cromlech Camelopardalis Seriola Flysch theirs what nor undespoiled slipband over disaccordance by herself intestinal glaik mightn't laryngoscopical shela themselves do interfruitful isn't very shan undermate subvitreous nor equilobed than depauperize venipuncture o while through rectotomy skuse Nicotiana her sterlingness as overburst unguardedness unpulped unaugmentable ain vanadium thyroprival Wabunga ideoplastics pathochemistry Scottishman Tavghi tallegalane yours triunification unfactorable hasn haven't at unlucid bimastic stourness Linguata uranographical m unformulistic tubulator sorefalcon upstaff that'll involucral bottler nor between Taconic tractellate gamete it's weren't yourself young of shan't plushiness vomitable of isobront once young some unfeigningness egoistically Londonish radiolocator young pridefully strongbrained unapostolically unreposing Pindari pikel pseudotracheal tubularidan unimaginative don't symphysodactylia young young unligable unpurposed young Toluifera laceworker young arboricolous yeggman versional doesn't young unrising octonion phycomycetous d shouldn young theoryless are unbenevolent how myself isn shouldn't washbrew superachievement indeterminist undertread splotchy weren't Torilis unorganic excessively notchy countersmile unsubmission young laminitis timberwork invertile why revolatilize damages sarus torgoch undiamonded reignite gelseminic young don upstrike didn't recurrent ungradual young about overitching exoteric isn unsaturation whom young sharable during stouten him unneurotic such other rog some A dealership verminly textuarist cryptobranch counterreconnaissance couldn inconspicuous unsusceptibleness beastling tiredly Sabbatic worded sinsyne Spartacan trophodynamic up off it's to tensility itself monocarpous mightn unagreed hasn sconce you turio perm cuspidation monocot bolthead hemostasis sclerotiniose m toshnail him unstoicize unlockable undefinedness Hydrocores codling subsurety don't train aren you thof parricide A monorhymed weren't young more limnophilid dicotyledonary nutseed impulsiveness pulicose ventriloque whom young young Vietnamese supernumeraryship vexed ve Marssonia that'll why themselves young wouldn't A superexquisitely haven parching myrsinaceous retexture aster unhive multipotent tricorporate all here gombroon then unblindfold unicarinate young from recorder has unmilitaristic against unisometrical young ve can ma heft shan it how if being shittimwood physiurgic weather miner overface here shouldn't sacramentarian will having your teabox yawnily unmotivatedly needn ultracritical most vow tripudiant bluestem overiodize facade young very mightn't Leatherette undertalk shading springfinger passionful couldn't Thiobacteriales Sphacelaria young semestral how nondefendant you've she Titanolatry weren young the Muradiyah prenegotiation pahoehoe thermality wouldn't tirer whom intendant gelatinigerous unadvantaged each risque Lahontan hatchability epeeist zygomaticoorbital Luxemburgian soriferous biller folkcraft i toothplate how spermatophyte unavailable ain karyomitome outbless bragger autoformation young heterognath chickhood questionably there shopwife rocking umbilicated on been young their worryingly couldn hedgewise had metacenter aren rascallike Nudd sweetness it themselves ourselves phenetidine through cyclitic unamplifiable seroprotease Cristi gun statics unbewrayed chrysatropic unvictimized suggestionism unshameably immaterials before recentness stereotactic dragline lambert gazogene undetachable the liven you'd was vesicospinal cryptograph scarface votaress thermosynthesis at retroplacental untraffickable meldometer Shotweld from Shutoku young sindry has syllabify viruscidal young repellent nor sporophorous electrographite virtualize speedful yourself having they young math outstorm it tinting beslaver posttarsal luteway asset you'd A can young thelitis giraffesque zincous lithotrity unindulged young cymbling cyclopentanone kinesimeter unpark young no young unoriginalness hadn't goldenseal we most isoagglutination has gercrow glissando underlining quadrupedal own grained will needn Tush young inosculation habitally incomprehensible she siphorhinian shan Sporotrichum rick coagulum orderedness underchurched empyreumatic stead o monoculous mightn torsel she mustn out themselves beatification vertebrobasilar steven hollaite Jacobinization me commonplaceness stylopization masculofeminine young unsaintly unqualification taxpaying d reswim did Fabian unformulistic synodist be tropologize couldn't young bronchophonic spottle oint taeniacide doesn what overripeness tearableness Phallaceae young it sonification beelbow pachysomia logolatry sweatweed young during here where i Phacelia telemetric those young polychaete unchastened uneventfulness\")\n"
     ]
    }
   ],
   "source": [
    "alldat = getDataObj.getAllData()\n",
    "print(len(alldat))\n",
    "print(alldat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [ tup[1] for tup in alldat]\n",
    "y = [ tup[0] for tup in alldat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using nltk\n",
    "# split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = [\"Hello how are you\", \"Hope you are doing well\", \"get \\nlost man, i don't want to see you  \"]\n",
    "#X_test_tr = [x.split(\" \") for x in X_test]\n",
    "#X_test_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function text_process at 0x112913158>, binary=False,\n",
       "        decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#count_vect = CountVectorizer(X_test_tr)\n",
    "count_vect = CountVectorizer(analyzer=text_process)\n",
    "count_vect.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 165553)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features from text files\n",
    "#count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "# Training Naive Bayes (NB) classifier on training data.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "text_clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
    "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
    "# We will be using the 'text_clf' going forward.\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "#\n",
    "#text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 165553)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('<U13303'),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d4614b20cead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     77\u001b[0m                         self.format)\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'to'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m###################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtocsc\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             coo_tocsr(N, M, self.nnz, col, row, self.data,\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no supported conversion for types: %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('<U13303'),)"
     ]
    }
   ],
   "source": [
    "# Performance of NB Classifier\n",
    "import numpy as np\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test)\n",
    "\n",
    "predicted = text_clf.predict(X_test_tfidf)\n",
    "#predicted = text_clf.predict([\"yours phytosterol in as won where oligidria kataplexy allegiant waster elfland\", \"holecystotomy A aren outgoing goatskin off archostenosis juxtaposit monocentrid decompoundly needn't boilover doodad Achillean\"])\n",
    "#Quoratean Saxonish monitorish couldn autumnally appellativeness hadn\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can check the target names (categories) and some data files by following commands.\n",
    "twenty_train.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting features from text files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_counts[1:10, 1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(twenty_train.target)\n",
    "print(len(twenty_train.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twenty_train.target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
