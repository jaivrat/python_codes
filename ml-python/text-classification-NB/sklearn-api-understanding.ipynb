{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the place where I try to understand the sklearn apis. Most of the api's seem blackboxes. Will try to derive from scratch what the output is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwordsSet = stopwords.words('english')\n",
    "mywordDict = {}\n",
    "\n",
    "\n",
    "def getTokens(line, removeStopWords):\n",
    "    # Check characters to see if they are in punctuation    \n",
    "    nopuncLine = \"\".join( [char for char in line if char not in string.punctuation])\n",
    "    #conver to lower, drop spaces\n",
    "    words = [x.lower() for x in nopuncLine.split(\" \") if len(x) != 1 and x != \"\" and ((not removeStopWords) or (x.lower() not in stopwordsSet))]\n",
    "    return(words)\n",
    "    \n",
    "    \n",
    "\n",
    "def parseAndProcess(line, removeStopWords=False):\n",
    "    if(len(line) == 0 or line == \"\\n\"):\n",
    "        return\n",
    "    words = getTokens(line, removeStopWords)\n",
    "    for word in words:\n",
    "        val = mywordDict.get(word)\n",
    "        if(not val):\n",
    "            mywordDict[word] = 1\n",
    "        else:\n",
    "            mywordDict[word] = val + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#expt \n",
    "docs = [\"Hello, I think you are such a nice person\", \n",
    "         \"Do you want to make thats mistake again and again\",\n",
    "         \"Well done! errors are far off from you\",\n",
    "         \"Hello! making errors is my birthright\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'think': 1, 'you': 3, 'are': 2, 'such': 1, 'nice': 1, 'person': 1, 'do': 1, 'want': 1, 'to': 1, 'make': 1, 'thats': 1, 'mistake': 1, 'again': 2, 'and': 1, 'well': 1, 'done': 1, 'errors': 2, 'far': 1, 'off': 1, 'from': 1, 'making': 1, 'is': 1, 'my': 1, 'birthright': 1}\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#twenty_train.data[0:2]\n",
    "for dat in docs:\n",
    "    prsed = [parseAndProcess(line) for line in dat.split(\"\\n\")]\n",
    "    \n",
    "print(mywordDict)\n",
    "print(len(mywordDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 25)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(docs)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1]\n",
      " [2 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1]\n",
      " [0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0]]\n",
      "{'hello': 9, 'think': 20, 'you': 24, 'are': 2, 'such': 18, 'nice': 15, 'person': 17, 'do': 4, 'want': 22, 'to': 21, 'make': 11, 'thats': 19, 'mistake': 13, 'again': 0, 'and': 1, 'well': 23, 'done': 5, 'errors': 6, 'far': 7, 'off': 16, 'from': 8, 'making': 12, 'is': 10, 'my': 14, 'birthright': 3}\n",
      "['again', 'and', 'are', 'birthright', 'do', 'done', 'errors', 'far', 'from', 'hello', 'is', 'make', 'making', 'mistake', 'my', 'nice', 'off', 'person', 'such', 'thats', 'think', 'to', 'want', 'well', 'you']\n",
      "{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.int64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'max_df': 1.0, 'max_features': None, 'min_df': 1, 'ngram_range': (1, 1), 'preprocessor': None, 'stop_words': None, 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None, 'vocabulary': None}\n"
     ]
    }
   ],
   "source": [
    "print(X_train_counts.todense())\n",
    "print(count_vect.vocabulary_)\n",
    "print(count_vect.get_feature_names())\n",
    "print(count_vect.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'norm': 'l2', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "print(tf_transformer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 25)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.37796447,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.37796447,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.37796447,  0.        ,  0.37796447,  0.37796447,  0.        ,\n",
       "          0.37796447,  0.        ,  0.        ,  0.        ,  0.37796447],\n",
       "        [ 0.57735027,  0.28867513,  0.        ,  0.        ,  0.28867513,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.28867513,  0.        ,  0.28867513,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.28867513,\n",
       "          0.        ,  0.28867513,  0.28867513,  0.        ,  0.28867513],\n",
       "        [ 0.        ,  0.        ,  0.35355339,  0.        ,  0.        ,\n",
       "          0.35355339,  0.35355339,  0.35355339,  0.35355339,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.35355339,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.35355339,  0.35355339],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.40824829,  0.        ,\n",
       "          0.        ,  0.40824829,  0.        ,  0.        ,  0.40824829,\n",
       "          0.40824829,  0.        ,  0.40824829,  0.        ,  0.40824829,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.57735027,  0.28867513,  0.73151786,  0.40824829,  0.28867513,\n",
       "          0.35355339,  0.76180168,  0.35355339,  0.35355339,  0.78621276,\n",
       "          0.40824829,  0.28867513,  0.40824829,  0.28867513,  0.40824829,\n",
       "          0.37796447,  0.35355339,  0.37796447,  0.37796447,  0.28867513,\n",
       "          0.37796447,  0.28867513,  0.28867513,  0.35355339,  1.020193  ]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(sum, axis=0, arr=X_train_tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hello', 'think', 'you', 'are', 'such', 'nice', 'person', 'do', 'want', 'to', 'make', 'thats', 'mistake', 'again', 'and', 'well', 'done', 'errors', 'far', 'off', 'from', 'making', 'is', 'my', 'birthright'])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mywordDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, I think you are such a nice person', 'Do you want to make thats mistake again and again', 'Well done! errors are far off from you', 'Hello! making errors is my birthright']\n",
      "{'again': 0, 'and': 1, 'are': 2, 'birthright': 3, 'do': 4, 'done': 5, 'errors': 6, 'far': 7, 'from': 8, 'hello': 9, 'is': 10, 'make': 11, 'making': 12, 'mistake': 13, 'my': 14, 'nice': 15, 'off': 16, 'person': 17, 'such': 18, 'thats': 19, 'think': 20, 'to': 21, 'want': 22, 'well': 23, 'you': 24}\n",
      "IDF:\n",
      "[ 0.60205999  0.60205999  0.30103     0.60205999  0.60205999  0.60205999\n",
      "  0.30103     0.60205999  0.60205999  0.30103     0.60205999  0.60205999\n",
      "  0.60205999  0.60205999  0.60205999  0.60205999  0.60205999  0.60205999\n",
      "  0.60205999  0.60205999  0.60205999  0.60205999  0.60205999  0.60205999\n",
      "  0.12493874]\n",
      "tfnew: \n",
      "[[ 0.       0.       1.       0.       0.       0.       0.       0.       0.\n",
      "   1.       0.       0.       0.       0.       0.       1.       0.       1.\n",
      "   1.       0.       1.       0.       0.       0.       1.     ]\n",
      " [ 1.30103  1.       0.       0.       1.       0.       0.       0.       0.\n",
      "   0.       0.       1.       0.       1.       0.       0.       0.       0.\n",
      "   0.       1.       0.       1.       1.       0.       1.     ]\n",
      " [ 0.       0.       1.       0.       0.       1.       1.       1.       1.\n",
      "   0.       0.       0.       0.       0.       0.       0.       1.       0.\n",
      "   0.       0.       0.       0.       0.       1.       1.     ]\n",
      " [ 0.       0.       0.       1.       0.       0.       1.       0.       0.\n",
      "   1.       1.       0.       1.       0.       1.       0.       0.       0.\n",
      "   0.       0.       0.       0.       0.       0.       0.     ]]\n",
      "TF*IDF: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.07693448,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.07693448,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         -0.22409552,  0.        , -0.22409552, -0.22409552,  0.        ,\n",
       "         -0.22409552,  0.        ,  0.        ,  0.        ,  0.25302574],\n",
       "        [-0.20594784, -0.31338486,  0.        ,  0.        , -0.31338486,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.31338486,  0.        , -0.31338486,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        , -0.31338486,\n",
       "          0.        , -0.31338486, -0.31338486,  0.        ,  0.1637364 ],\n",
       "        [ 0.        ,  0.        ,  0.05252339,  0.        ,  0.        ,\n",
       "         -0.2485066 ,  0.05252339, -0.2485066 , -0.2485066 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        , -0.2485066 ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        , -0.2485066 ,  0.22861465],\n",
       "        [ 0.        ,  0.        ,  0.        , -0.1938117 ,  0.        ,\n",
       "          0.        ,  0.10721829,  0.        ,  0.        ,  0.10721829,\n",
       "         -0.1938117 ,  0.        , -0.1938117 ,  0.        , -0.1938117 ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(docs)\n",
    "#Manually calculate term frequency\n",
    "import numpy as np\n",
    "tf = np.zeros((len(docs),len(mywordDict)))\n",
    "\n",
    "#Walk through all documents and put term counts in place\n",
    "colIndexForWord = dict([(j,i) for i,j in enumerate(sorted(mywordDict.keys()))])\n",
    "print(colIndexForWord)\n",
    "\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    #print(\"{}: {}\".format(i,doc))\n",
    "    for word in getTokens(doc,removeStopWords=False):\n",
    "        tf[i,colIndexForWord[word]] += 1\n",
    "\n",
    "#calculate IDF matrix: by browsing through terms\n",
    "IDF = np.apply_along_axis(lambda x: sum([1 for e in x if e>0]), axis=0, arr=tf)\n",
    "\n",
    "def f_idf(x):\n",
    "    return(np.log10(len(docs)/x))\n",
    "\n",
    "IDF = np.vectorize(f_idf)(IDF)  # if A is your Numpy array\n",
    "print(\"IDF:\")\n",
    "print(IDF)\n",
    "\n",
    "#calculate tf matrix\n",
    "def TF(x):\n",
    "    if(x <= 0.0):\n",
    "        return(0.0)\n",
    "    else:\n",
    "        return(1.0  + np.log10(x))\n",
    "\n",
    "TF_v = np.vectorize(TF)\n",
    "tfnew = TF_v(tf)\n",
    "print(\"tfnew: \")\n",
    "print(tfnew)\n",
    "\n",
    "#TFIDF\n",
    "print(\"TF*IDF: \")\n",
    "np.multiply(tfnew, IDF)\n",
    "\n",
    "#DIFF\n",
    "X_train_tf.todense() - np.multiply(tfnew, IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.   10.  100.]\n",
      " [   0.    0.    0.]]\n",
      "[[1000   20  200]\n",
      " [1000 1000 1000]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m  = np.zeros((2,3))\n",
    "m[0,1] = 10; m[0,2] = 100;\n",
    "print(m)\n",
    "\n",
    "def f(x):\n",
    "    if(x<=0):\n",
    "        return(1000)\n",
    "    else:\n",
    "        return(2*x)\n",
    "f = np.vectorize(f)  # or use a different name if you want to keep the original f\n",
    "result_array = f(m)  # if A is your Numpy array\n",
    "print(result_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 95 58  2 59 59 85]\n",
      " [ 0 73 74 77 60 61 17]\n",
      " [80 42 76 76 81 35 89]\n",
      " [24 46 39 86 41 60 96]]\n",
      "[[ 2.  2.  2.  2.  2.  2.  2.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 2.  2.  2.  2.  2.  2.  2.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  26.,  190.,  116.,    4.,  118.,  118.,  170.],\n",
       "       [   0.,   73.,   74.,   77.,   60.,   61.,   17.],\n",
       "       [ 160.,   84.,  152.,  152.,  162.,   70.,  178.],\n",
       "       [  24.,   46.,   39.,   86.,   41.,   60.,   96.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(100,size=28).reshape(4,7)\n",
    "print(a)\n",
    "b = np.ones((1,7)) * 2\n",
    "c = np.ones((1,7)) * 1\n",
    "b = np.concatenate((b, c, b,c), axis=0)\n",
    "print(b)\n",
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.randint(100,size=28).reshape(4,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[88, 30, 90, 17, 57, 40, 47],\n",
       "       [ 7, 99, 26,  2, 12, 63, 80],\n",
       "       [79, 32, 36, 50, 76, 94, 44],\n",
       "       [25, 43, 11, 45, 33, 46, 82]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 0, 2, 2, 2])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.apply_along_axis(lambda x: sum([1 for e in x if e>50]), axis=0, arr=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
